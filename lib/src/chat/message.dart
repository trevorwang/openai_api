import 'package:freezed_annotation/freezed_annotation.dart';
import 'package:openai_api/src/chat/completion.dart';

part 'message.freezed.dart';
part 'message.g.dart';

@freezed
class SystemMessage with _$SystemMessage {
  const factory SystemMessage({
    required String content,
    @Default(ChatMessageRole.system) ChatMessageRole role,
    String? name,
  }) = _SystemMessage;

  factory SystemMessage.fromJson(Map<String, dynamic> json) =>
      _$SystemMessageFromJson(json);
}

@freezed
class AssistantMessage with _$AssistantMessage {
  const factory AssistantMessage({
    /// The contents of the assistant message.
    required String content,

    /// The role of the messages author, in this case assistant.
    @Default(ChatMessageRole.assistant) ChatMessageRole role,

    /// An optional name for the participant. Provides the model information
    ///  to differentiate between participants of the same role.
    String? name,

    /// The tool calls generated by the model, such as function calls.
    List<MessageToolCall>? toolCalls,
  }) = _AssistantMessage;

  factory AssistantMessage.fromJson(Map<String, dynamic> json) =>
      _$AssistantMessageFromJson(json);
}

@freezed
class ToolMessage with _$ToolMessage {
  const factory ToolMessage({
    /// The contents of the tool message.
    required String content,

    /// The role of the messages author, in this case assistant.
    @Default(ChatMessageRole.tool) ChatMessageRole role,

    /// Tool call that this message is responding to.
    required String toolCallId,
  }) = _ToolMessage;

  factory ToolMessage.fromJson(Map<String, dynamic> json) =>
      _$ToolMessageFromJson(json);
}

@freezed
class UserMessage with _$UserMessage {
  const factory UserMessage({
    /// The contents of the user message.
    /// Must be one of Text content or
    ///
    /// An array of content parts with a defined type, each can be of type `text`
    /// or `image_url` when passing in images. You can pass multiple images by
    /// adding multiple image_url content parts. Image input is only supported
    /// when using the `gpt-4-visual-preview` model.
    required dynamic content,

    /// The role of the messages author, in this case user.
    @Default(ChatMessageRole.user) ChatMessageRole role,

    /// An optional name for the participant. Provides the model information
    ///  to differentiate between participants of the same role.
    String? name,
  }) = _UserMessage;

  factory UserMessage.fromJson(Map<String, dynamic> json) =>
      _$UserMessageFromJson(json);
}

@JsonEnum(fieldRename: FieldRename.snake)
enum MessageContentType {
  text,
  imageUrl,
}

@freezed
class TextContent with _$TextContent {
  const factory TextContent({
    /// The text content of the message.
    required String text,
    @Default(MessageContentType.text) MessageContentType type,
  }) = _TextContent;
  factory TextContent.fromJson(Map<String, dynamic> json) =>
      _$TextContentFromJson(json);
}

@freezed
class ImageContent with _$ImageContent {
  const factory ImageContent({
    /// The url of the image.
    required ImageUrl imageUrl,
    @Default(MessageContentType.imageUrl) MessageContentType type,
  }) = _ImageContent;

  factory ImageContent.fromJson(Map<String, dynamic> json) =>
      _$ImageContentFromJson(json);
}

@freezed
class ImageUrl with _$ImageUrl {
  const factory ImageUrl({
    /// Either a URL of the image or the base64 encoded image data.
    required String url,

    /// Specifies the detail level of the image.
    @Default("auto") String detail,
  }) = _ImageUrl;

  factory ImageUrl.fromJson(Map<String, dynamic> json) =>
      _$ImageUrlFromJson(json);
}
